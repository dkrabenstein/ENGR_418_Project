{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe1ae31",
   "metadata": {},
   "source": [
    "ENGR 418 - Group 33\n",
    "Matthew Ofina - 84790435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9399f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# this cell imports necassary libraries\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4320ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# this cell defines a function for retreiving data from a folder\n",
    "# ============================================================\n",
    "\n",
    "# converts images from folder into usable training/testing data\n",
    "def get_data(folder_path, im_width):\n",
    "    # lists all files in a given folder\n",
    "    file_names = os.listdir(folder_path)\n",
    "    # initializes arrays for data\n",
    "    x = np.empty((len(file_names), im_width**2))\n",
    "    y = np.empty((len(file_names), 1))\n",
    "    # iterates through every file in the folder\n",
    "    for i in range(len(file_names)):\n",
    "        #retrieves image file and label\n",
    "        file_name = file_names[i]\n",
    "        file_path = folder_path + file_name\n",
    "        label = get_label(file_name)\n",
    "        #converts image to usable data\n",
    "        im = Image.open(file_path).convert('L')\n",
    "        im = im.resize((im_width, im_width))\n",
    "        im_array = np.asarray(im)\n",
    "        #adds image data to data array\n",
    "        x[i,:] = im_array.reshape(1, -1)\n",
    "        y[i,0] = label\n",
    "    return x, y\n",
    "\n",
    "# returns the label of an image based on the filename\n",
    "def get_label(file_name):\n",
    "    if file_name[0] == '2':\n",
    "      label = 0\n",
    "    elif file_name[0] == 'c':\n",
    "      label = 1\n",
    "    elif file_name[0] == 'r':\n",
    "      label = 2\n",
    "    elif file_name[0] == 's':\n",
    "      label = 3\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9aaea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# this cell defines a function for retreiving data from a folder\n",
    "# ============================================================\n",
    "\n",
    "def train_function(path, im_width=64):\n",
    "    # uses get_data function to retrieve data\n",
    "    x_train, y_train = get_data(path, im_width)\n",
    "\n",
    "    # defines a logistic regressiion function with l2 norm and lambda=10 for regularization\n",
    "    model = LogisticRegression(penalty='l2', C=0.1)\n",
    "\n",
    "    # trains the model with the selected training data\n",
    "    model.fit(x_train,y_train)\n",
    "\n",
    "    # generate predicted labels for testset\n",
    "    y_pred = model.predict(x_train)\n",
    "\n",
    "    # get and print confusion matrix and accuracy score for train set\n",
    "    print(f\"Confusion matrix training:\\n{confusion_matrix(y_train,y_pred)}\\n\")\n",
    "    print(f\"Accuracy score training:\\n{accuracy_score(y_train,y_pred)}\\n\")\n",
    "    \n",
    "\n",
    "    # returns model to use for testing\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "829f02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test function required for grading\n",
    "# ============================================================\n",
    "\n",
    "def test_function(path, model, im_width=64):\n",
    "    # uses get_data function to retrieve data\n",
    "    x_test, y_test = get_data(path, im_width)\n",
    "\n",
    "    # generate predicted labels for testset\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # get and print confusion matrix and accuracy score for test set\n",
    "    print(f\"Confusion matrix testing:\\n {confusion_matrix(y_test,y_pred)}\\n\")\n",
    "    print(f\"Accuracy testing:\\n {accuracy_score(y_test,y_pred)}\\n\")\n",
    "\n",
    "    return y_test, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c22dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Uni\\Year4\\ENGR_418\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1406: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "d:\\Uni\\Year4\\ENGR_418\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix training:\n",
      "[[27  0  0  0]\n",
      " [ 0 27  0  0]\n",
      " [ 0  0 27  0]\n",
      " [ 0  0  0 27]]\n",
      "\n",
      "Accuracy score training:\n",
      "1.0\n",
      "\n",
      "Confusion matrix testing:\n",
      " [[11 10  4  2]\n",
      " [ 4 11  6  6]\n",
      " [ 2  1 23  1]\n",
      " [ 1  9  8  9]]\n",
      "\n",
      "Accuracy testing:\n",
      " 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Trains and tests model\n",
    "# ============================================================\n",
    "\n",
    "# input desired image width and folder paths\n",
    "im_width = 64\n",
    "train_folder_path = 'Lego_dataset_2/training/'\n",
    "test_folder_path = 'Lego_dataset_2/testing/'\n",
    "\n",
    "# train\n",
    "model = train_function(train_folder_path, im_width)\n",
    "# test\n",
    "_,_ = test_function(test_folder_path, model, im_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================================\n",
    "# Study the performance of training and testing as a function of the number of inputs\n",
    "# ====================================================================================\n",
    "\n",
    "inputs = [8, 11, 16, 22, 32, 45, 64]\n",
    "\n",
    "# Determine the confusion matrix and accuracy for varying input sizes\n",
    "for size in inputs:\n",
    "    print(f\"\\n{size}x{size} image with {size**2} inputs\\n\")\n",
    "    model = train_function(train_folder_path, size)\n",
    "    y_test, y_pred = test_function(test_folder_path, model, size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# For the inputs of 64, 121, 256, 484, 1024, 2025, and 4096, the following accuracy\n",
    "# results for the testing set were obtained to be 88.89%, 94.44%, 94.44%, 95.83%, 94.44%,\n",
    "# 98.61%, and 95.83%. This shows that more data does not always necessarily mean more \n",
    "# useful information. The additional data could be introducing noise and irrelevant\n",
    "# features. Another reason for accuracy fluctation could be due to underfitting and \n",
    "# overfitting.\n",
    "# ========================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a2c42",
   "metadata": {},
   "source": [
    "The following code is the new feature engineering algorithm for stage 2 (major AI help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9dc1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 1: Group Information and Imports\n",
    "# =========================================================================================\n",
    "# Group Number: [Insert Group Number Here]\n",
    "# Group Members:\n",
    "#   - [Name 1] (ID: [ID 1])\n",
    "#   - [Name 2] (ID: [ID 2])\n",
    "#   - [Name 3] (ID: [ID 3])\n",
    "#\n",
    "# Description:\n",
    "# This notebook implements Stage 2 of the project.\n",
    "# 1. Task 1: Evaluation of the Stage 1 (raw pixel) model on the new dataset.\n",
    "# 2. Task 2: Development of a new model using Feature Engineering to handle \n",
    "#    rotation and translation.\n",
    "#\n",
    "# Feature Engineering Library: OpenCV (cv2)\n",
    "# Features Used: Geometric properties (Area, Solidity, Aspect Ratio) and Hu Moments.\n",
    "# =========================================================================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac4e41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 2: Helper Functions (Label Extraction & Feature Engineering with OpenCV)\n",
    "# =========================================================================================\n",
    "\n",
    "def get_label(file_name):\n",
    "    \"\"\"\n",
    "    Determines the class label based on the filename.\n",
    "    Mappings:\n",
    "    - '2...' -> 0 (Rectangle 2x4)\n",
    "    - 'c...' -> 1 (Circle 2x2)\n",
    "    - 'r...' -> 2 (Small Rectangle 2x1)\n",
    "    - 's...' -> 3 (Square 2x2)\n",
    "    \"\"\"\n",
    "    if file_name.startswith('2'):\n",
    "        return 0\n",
    "    elif file_name.startswith('c'):\n",
    "        return 1\n",
    "    elif file_name.startswith('r'):\n",
    "        return 2\n",
    "    elif file_name.startswith('s'):\n",
    "        return 3\n",
    "    return 0 # Default fallback\n",
    "\n",
    "def extract_engineered_features_opencv(im_array):\n",
    "    \"\"\"\n",
    "    Extracts rotation and translation invariant features using OpenCV.\n",
    "    \n",
    "    Steps:\n",
    "    1. Preprocessing (Inversion, Otsu Thresholding, Morphological Closing).\n",
    "    2. Contour Detection (finding the object).\n",
    "    3. Geometric Feature Calculation (Area, Solidity, Extent, Aspect Ratio, Eccentricity).\n",
    "    4. Hu Moments Calculation (7 invariant moments).\n",
    "    \n",
    "    Returns:\n",
    "        np.array: A 1D array of approx 13 features.\n",
    "    \"\"\"\n",
    "    # 1. Preprocessing\n",
    "    # Ensure input is uint8\n",
    "    if im_array.dtype != np.uint8:\n",
    "        im_array = im_array.astype(np.uint8)\n",
    "\n",
    "    # Invert image: The project images typically have dark objects on light backgrounds.\n",
    "    # OpenCV contours work best on White Objects on Black Backgrounds.\n",
    "    inv_im = cv2.bitwise_not(im_array)\n",
    "    \n",
    "    # Thresholding (Otsu's method automatically finds the best threshold)\n",
    "    _, binary = cv2.threshold(inv_im, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Morphological closing to fill small holes/noise inside the object\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # 2. Find Contours\n",
    "    # RETR_EXTERNAL retrieves only the extreme outer contours\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # If image is blank or no contour found, return zero vector\n",
    "    if not contours:\n",
    "        return np.zeros(13)\n",
    "    \n",
    "    # Assume the largest contour is the Lego piece (filters out small noise)\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # 3. Geometric Features\n",
    "    area = cv2.contourArea(cnt)\n",
    "    perimeter = cv2.arcLength(cnt, True)\n",
    "    \n",
    "    # Solidity: Ratio of Area to Convex Hull Area\n",
    "    hull = cv2.convexHull(cnt)\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    solidity = area / hull_area if hull_area > 0 else 0\n",
    "        \n",
    "    # Extent: Ratio of Area to Bounding Rectangle Area\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    rect_area = w * h\n",
    "    extent = area / rect_area if rect_area > 0 else 0\n",
    "        \n",
    "    # Aspect Ratio & Eccentricity (using Fitted Ellipse)\n",
    "    # fitEllipse requires at least 5 points\n",
    "    if len(cnt) >= 5:\n",
    "        (x_center, y_center), (MA, ma), angle = cv2.fitEllipse(cnt)\n",
    "        # fitEllipse returns (MA, ma) as (minor_axis, major_axis) or vice versa depending on orientation\n",
    "        # We sort them to ensure consistent math\n",
    "        axes = sorted([MA, ma])\n",
    "        minor_axis, major_axis = axes[0], axes[1]\n",
    "        \n",
    "        if major_axis > 0:\n",
    "            aspect_ratio = major_axis / minor_axis if minor_axis > 0 else 0\n",
    "            # Eccentricity = sqrt(1 - (b/a)^2)\n",
    "            eccentricity = np.sqrt(1 - (minor_axis / major_axis)**2)\n",
    "        else:\n",
    "            aspect_ratio, eccentricity = 0, 0\n",
    "    else:\n",
    "        # Fallback for shapes too small to fit ellipse\n",
    "        aspect_ratio, eccentricity = 0, 0\n",
    "\n",
    "    # 4. Hu Moments (7 features invariant to scale, translation, rotation)\n",
    "    moments = cv2.moments(cnt)\n",
    "    hu = cv2.HuMoments(moments).flatten()\n",
    "    \n",
    "    # Combine features into single vector\n",
    "    features = np.array([\n",
    "        area, perimeter, eccentricity, solidity, extent, \n",
    "        aspect_ratio, \n",
    "        hu[0], hu[1], hu[2], hu[3], hu[4], hu[5], hu[6]\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def get_data_stage2(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all images in a folder and extracts engineered features using OpenCV.\n",
    "    \"\"\"\n",
    "    # Filter for image files\n",
    "    file_names = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    \n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Load Image\n",
    "        # We use PIL to open and convert to array to remain consistent with file handling,\n",
    "        # but we immediately convert to OpenCV compatible format.\n",
    "        im = Image.open(file_path).convert('L')\n",
    "        im_array = np.array(im)\n",
    "        \n",
    "        # Extract Features\n",
    "        feats = extract_engineered_features_opencv(im_array)\n",
    "        lbl = get_label(file_name)\n",
    "        \n",
    "        features_list.append(feats)\n",
    "        labels_list.append(lbl)\n",
    "        \n",
    "    return np.array(features_list), np.array(labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3634509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 3: Training Function (Stage 2)\n",
    "# =========================================================================================\n",
    "\n",
    "def training_function(path, penalty='l2', C=1.0):\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression model using engineered features.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to training folder.\n",
    "    \n",
    "    Returns:\n",
    "        model (Pipeline): Trained scikit-learn pipeline (Scaler + Classifier).\n",
    "    \"\"\"\n",
    "    print(f\"Loading training data from: {path}\")\n",
    "    X_train, y_train = get_data_stage2(path)\n",
    "    \n",
    "    print(f\"Extracted features shape: {X_train.shape}\")\n",
    "    \n",
    "    # Create Pipeline\n",
    "    # 1. StandardScaler: Normalize features (Area is ~1000s, Hu Moments are ~0.001)\n",
    "    # 2. LogisticRegression: The classifier\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(penalty=penalty, C=C, max_iter=2000, multi_class='multinomial'))\n",
    "    ])\n",
    "    \n",
    "    # Train\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on Training Data\n",
    "    y_pred = pipe.predict(X_train)\n",
    "    print(\"\\n--- Training Performance (Stage 2) ---\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_train, y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_train, y_pred):.4f}\")\n",
    "    \n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "487f4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 4: Testing Function (Stage 2)\n",
    "# =========================================================================================\n",
    "\n",
    "def testing_function(path, model):\n",
    "    \"\"\"\n",
    "    Tests the trained pipeline on new data.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to testing folder.\n",
    "        model: Trained pipeline.\n",
    "        \n",
    "    Returns:\n",
    "        y_test, y_pred\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading testing data from: {path}\")\n",
    "    X_test, y_test = get_data_stage2(path)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\n--- Testing Performance (Stage 2) ---\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    return y_test, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98937fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== TASK 1: STAGE 1 LEGACY MODEL EVALUATION ==========\n",
      "Stage 1 Model (Raw Pixels) Results on New Dataset:\n",
      "Accuracy: 0.5463\n",
      "Confusion Matrix:\n",
      "[[11 12  3  1]\n",
      " [ 4 15  6  2]\n",
      " [ 2  1 22  2]\n",
      " [ 2  8  6 11]]\n",
      "Observation: Raw pixels are not rotation invariant.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================================\n",
    "# Cell 5: Task 1 - Evaluate Stage 1 Code on New Dataset\n",
    "# =========================================================================================\n",
    "# This functions acts as a baseline. It uses the Stage 1 method (Raw Pixels) on the new\n",
    "# rotated/shifted data. Low accuracy is expected.\n",
    "\n",
    "def get_data_stage1_legacy(folder_path, im_width):\n",
    "    # Reads images and flattens raw pixels (no feature engineering)\n",
    "    file_names = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg'))]\n",
    "    x = np.empty((len(file_names), im_width**2))\n",
    "    y = np.empty((len(file_names), 1))\n",
    "    for i, fname in enumerate(file_names):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        label = get_label(fname)\n",
    "        \n",
    "        im = Image.open(fpath).convert('L')\n",
    "        im = im.resize((im_width, im_width))\n",
    "        im_array = np.asarray(im)\n",
    "        \n",
    "        x[i,:] = im_array.reshape(1, -1)\n",
    "        y[i,0] = label\n",
    "    return x, y.ravel()\n",
    "\n",
    "print(\"========== TASK 1: STAGE 1 LEGACY MODEL EVALUATION ==========\")\n",
    "# Define Paths (UPDATE THESE TO YOUR ACTUAL FOLDER PATHS)\n",
    "TRAIN_PATH = 'Lego_dataset_2/training/'\n",
    "TEST_PATH  = 'Lego_dataset_2/testing/'\n",
    "\n",
    "try:\n",
    "    # 1. Train Stage 1 Model on New Data (Raw Pixels)\n",
    "    im_width = 64\n",
    "    X_train_raw, y_train_raw = get_data_stage1_legacy(TRAIN_PATH, im_width)\n",
    "    \n",
    "    model_s1 = LogisticRegression(penalty='l2', C=0.1, max_iter=2000)\n",
    "    model_s1.fit(X_train_raw, y_train_raw)\n",
    "    \n",
    "    # 2. Test Stage 1 Model\n",
    "    X_test_raw, y_test_raw = get_data_stage1_legacy(TEST_PATH, im_width)\n",
    "    y_pred_s1 = model_s1.predict(X_test_raw)\n",
    "    \n",
    "    print(\"Stage 1 Model (Raw Pixels) Results on New Dataset:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test_raw, y_pred_s1):.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_test_raw, y_pred_s1)}\")\n",
    "    print(\"Observation: Raw pixels are not rotation invariant.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not run Task 1 (check paths or data): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d0ad18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TASK 2: STAGE 2 FEATURE ENGINEERING MODEL (OpenCV) ==========\n",
      "Loading training data from: Lego_dataset_2/training/\n",
      "Extracted features shape: (108, 13)\n",
      "\n",
      "--- Training Performance (Stage 2) ---\n",
      "Confusion Matrix:\n",
      "[[27  0  0  0]\n",
      " [ 0 25  0  2]\n",
      " [ 0  0 26  1]\n",
      " [ 0  7  1 19]]\n",
      "Accuracy: 0.8981\n",
      "\n",
      "Loading testing data from: Lego_dataset_2/testing/\n",
      "\n",
      "--- Testing Performance (Stage 2) ---\n",
      "Confusion Matrix:\n",
      "[[17  8  2  0]\n",
      " [ 0 24  0  3]\n",
      " [ 1  0 24  2]\n",
      " [ 0  9  1 17]]\n",
      "Accuracy: 0.7593\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================================\n",
    "# Cell 6: Task 2 - Execute Stage 2 Training and Testing (OpenCV Feature Engineering)\n",
    "# =========================================================================================\n",
    "\n",
    "print(\"\\n========== TASK 2: STAGE 2 FEATURE ENGINEERING MODEL (OpenCV) ==========\")\n",
    "\n",
    "try:\n",
    "    # 1. Train the new model\n",
    "    # training_function handles data loading, feature extraction (OpenCV), \n",
    "    # scaling, and model fitting.\n",
    "    stage2_model = training_function(TRAIN_PATH)\n",
    "\n",
    "    # 2. Test the new model\n",
    "    y_test, y_pred = testing_function(TEST_PATH, stage2_model)\n",
    "    \n",
    "except Exception as e:\n",
    "     print(f\"Could not run Task 2 (check paths or data): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8086aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning & Feature Selection Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 2: Robust Feature Engineering (Stage 2 - Final)\n",
    "# =========================================================================================\n",
    "\n",
    "def get_label(file_name):\n",
    "    if file_name.startswith('2'): return 0\n",
    "    elif file_name.startswith('c'): return 1\n",
    "    elif file_name.startswith('r'): return 2\n",
    "    elif file_name.startswith('s'): return 3\n",
    "    return 0\n",
    "\n",
    "def extract_engineered_features_opencv(im_array):\n",
    "    \"\"\"\n",
    "    Extracts geometric features using minAreaRect for rotation stability.\n",
    "    \"\"\"\n",
    "    if im_array.dtype != np.uint8:\n",
    "        im_array = im_array.astype(np.uint8)\n",
    "\n",
    "    # 1. Preprocessing\n",
    "    inv_im = cv2.bitwise_not(im_array)\n",
    "    _, binary = cv2.threshold(inv_im, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if not contours: return np.zeros(13)\n",
    "    \n",
    "    # Pick largest contour\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # --- Geometric Features ---\n",
    "    area = cv2.contourArea(cnt)\n",
    "    perimeter = cv2.arcLength(cnt, True)\n",
    "    \n",
    "    # Compactness (P^2 / A) \n",
    "    compactness = (perimeter ** 2) / area if area > 0 else 0\n",
    "\n",
    "    # Solidity (Area / Convex Hull Area)\n",
    "    hull = cv2.convexHull(cnt)\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    solidity = area / hull_area if hull_area > 0 else 0\n",
    "    \n",
    "    # --- ROTATION INVARIANT ASPECT RATIO (Critical) ---\n",
    "    rect = cv2.minAreaRect(cnt) \n",
    "    (center), (width, height), angle = rect\n",
    "    \n",
    "    # Aspect Ratio (Long / Short)\n",
    "    dims = sorted([width, height])\n",
    "    short, long = dims[0], dims[1]\n",
    "    aspect_ratio = long / short if short > 0 else 0\n",
    "    \n",
    "    # Rectangularity (Area / Rotated Box Area)\n",
    "    rect_area = width * height\n",
    "    rectangularity = area / rect_area if rect_area > 0 else 0\n",
    "\n",
    "    # --- Hu Moments (Log Scale) ---\n",
    "    moments = cv2.moments(cnt)\n",
    "    hu = cv2.HuMoments(moments).flatten()\n",
    "    hu_log = [ -1 * np.sign(h) * np.log10(np.abs(h) + 1e-7) for h in hu ]\n",
    "    \n",
    "    # Feature Vector\n",
    "    features = np.array([\n",
    "        area, perimeter, compactness, solidity, rectangularity, aspect_ratio, \n",
    "        hu_log[0], hu_log[1], hu_log[2], hu_log[3], hu_log[4], hu_log[5], hu_log[6]\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def get_data_stage2(folder_path):\n",
    "    file_names = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg'))]\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        im = Image.open(file_path).convert('L')\n",
    "        im_array = np.array(im)\n",
    "        feats = extract_engineered_features_opencv(im_array)\n",
    "        lbl = get_label(file_name)\n",
    "        features_list.append(feats)\n",
    "        labels_list.append(lbl)\n",
    "        \n",
    "    return np.array(features_list), np.array(labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b3574c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 3: Training Function (L1 Lasso Regularization)\n",
    "# =========================================================================================\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "def training_function(path):\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "    1. Scaling\n",
    "    2. Poly Features (Degree 2) -> Generates high complexity\n",
    "    3. LogisticRegressionCV (L1 Penalty) -> Intelligently selects features\n",
    "    \"\"\"\n",
    "    print(f\"Loading training data from: {path}\")\n",
    "    X_train, y_train = get_data_stage2(path)\n",
    "    \n",
    "    # Pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        # Create non-linear features (interactions)\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        \n",
    "        # LogisticRegressionCV with L1 penalty automatically performs feature selection\n",
    "        # by shrinking coefficients of non-informative features to zero.\n",
    "        ('clf', LogisticRegressionCV(\n",
    "            cv=5, \n",
    "            penalty='l1',      # L1 = Lasso (Feature Selection)\n",
    "            solver='saga',     # SAGA solver supports L1 for multinomial\n",
    "            max_iter=10000,    # High iterations needed for convergence\n",
    "            multi_class='multinomial',\n",
    "            Cs=10              # Check 10 different regularization strengths\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train\n",
    "    print(\"Training with L1 Regularization (this may take 30-60 seconds)...\")\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Print best Regularization found\n",
    "    best_c = pipe.named_steps['clf'].C_[0]\n",
    "    print(f\"Best C (Inverse Regularization): {best_c:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = pipe.predict(X_train)\n",
    "    print(\"\\n--- Training Performance (L1 Optimized) ---\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_train, y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_train, y_pred):.4f}\")\n",
    "    \n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3f2dfe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 4: Testing Function\n",
    "# =========================================================================================\n",
    "\n",
    "def testing_function(path, model):\n",
    "    \"\"\"\n",
    "    Tests the trained pipeline on new data.\n",
    "    The pipeline automatically applies the same Scaling and Feature Selection.\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading testing data from: {path}\")\n",
    "    X_test, y_test = get_data_stage2(path)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\n--- Testing Performance (Stage 2) ---\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    return y_test, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d584c7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== STAGE 2: FEATURE ENGINEERING + SELECTION ==========\n",
      "Loading training data from: Lego_dataset_2/training/\n",
      "Training with L1 Regularization (this may take 30-60 seconds)...\n",
      "Best C (Inverse Regularization): 21.5443\n",
      "\n",
      "--- Training Performance (L1 Optimized) ---\n",
      "Confusion Matrix:\n",
      "[[27  0  0  0]\n",
      " [ 0 27  0  0]\n",
      " [ 0  0 27  0]\n",
      " [ 0  0  0 27]]\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Loading testing data from: Lego_dataset_2/testing/\n",
      "\n",
      "--- Testing Performance (Stage 2) ---\n",
      "Confusion Matrix:\n",
      "[[18  6  0  3]\n",
      " [ 0 22  0  5]\n",
      " [ 1  1 23  2]\n",
      " [ 0  4  0 23]]\n",
      "Accuracy: 0.7963\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================================\n",
    "# Cell 5: Main Execution\n",
    "# =========================================================================================\n",
    "\n",
    "# Define Paths (UPDATE THESE TO YOUR ACTUAL FOLDER PATHS)\n",
    "TRAIN_PATH = 'Lego_dataset_2/training/'\n",
    "TEST_PATH  = 'Lego_dataset_2/testing/'\n",
    "\n",
    "print(\"========== STAGE 2: FEATURE ENGINEERING + SELECTION ==========\")\n",
    "\n",
    "try:\n",
    "    # 1. Train Model (includes SFS)\n",
    "    stage2_model = training_function(TRAIN_PATH)\n",
    "\n",
    "    # 2. Test Model\n",
    "    y_test, y_pred = testing_function(TEST_PATH, stage2_model)\n",
    "    \n",
    "except Exception as e:\n",
    "     print(f\"Could not run Task (check paths or data): {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7617e",
   "metadata": {},
   "source": [
    "Different try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d565d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 1: Group Information and Imports\n",
    "# =========================================================================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning Imports\n",
    "from sklearn.svm import SVC                   # Support Vector Classifier\n",
    "from sklearn.model_selection import GridSearchCV # Automated Parameter Tuning\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d3277730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 2: Physics-Based Feature Engineering (5 Robust Features)\n",
    "# =========================================================================================\n",
    "\n",
    "def get_label(file_name):\n",
    "    if file_name.startswith('2'): return 0\n",
    "    elif file_name.startswith('c'): return 1\n",
    "    elif file_name.startswith('r'): return 2\n",
    "    elif file_name.startswith('s'): return 3\n",
    "    return 0\n",
    "\n",
    "def extract_engineered_features_opencv(im_array):\n",
    "    \"\"\"\n",
    "    Extracts ONLY the 5 physically descriptive features.\n",
    "    \"\"\"\n",
    "    if im_array.dtype != np.uint8:\n",
    "        im_array = im_array.astype(np.uint8)\n",
    "\n",
    "    # 1. Preprocessing\n",
    "    inv_im = cv2.bitwise_not(im_array)\n",
    "    _, binary = cv2.threshold(inv_im, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if not contours: return np.zeros(5)\n",
    "    \n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # --- 1. Area ---\n",
    "    area = cv2.contourArea(cnt)\n",
    "    \n",
    "    # --- 2. Compactness ---\n",
    "    perimeter = cv2.arcLength(cnt, True)\n",
    "    compactness = (perimeter ** 2) / area if area > 0 else 0\n",
    "\n",
    "    # --- 3. Solidity ---\n",
    "    hull = cv2.convexHull(cnt)\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    solidity = area / hull_area if hull_area > 0 else 0\n",
    "    \n",
    "    # --- 4. Aspect Ratio (Rotation Invariant) ---\n",
    "    rect = cv2.minAreaRect(cnt) \n",
    "    (center), (width, height), angle = rect\n",
    "    dims = sorted([width, height])\n",
    "    aspect_ratio = dims[1] / dims[0] if dims[0] > 0 else 0\n",
    "    \n",
    "    # --- 5. Rectangularity ---\n",
    "    rect_area = width * height\n",
    "    rectangularity = area / rect_area if rect_area > 0 else 0\n",
    "\n",
    "    features = np.array([area, compactness, solidity, aspect_ratio, rectangularity])\n",
    "    return features\n",
    "\n",
    "def get_data_stage2(folder_path):\n",
    "    file_names = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg'))]\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        im = Image.open(file_path).convert('L')\n",
    "        im_array = np.array(im)\n",
    "        feats = extract_engineered_features_opencv(im_array)\n",
    "        lbl = get_label(file_name)\n",
    "        features_list.append(feats)\n",
    "        labels_list.append(lbl)\n",
    "        \n",
    "    return np.array(features_list), np.array(labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 3: Training Function (Linear SVM for Generalization)\n",
    "# =========================================================================================\n",
    "\n",
    "def training_function(path):\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "    1. Scaling (StandardScaler)\n",
    "    2. Linear SVM\n",
    "    \"\"\"\n",
    "    print(f\"Loading training data from: {path}\")\n",
    "    X_train, y_train = get_data_stage2(path)\n",
    "    \n",
    "    # Pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(kernel='linear', probability=True))\n",
    "    ])\n",
    "    \n",
    "    # Grid Search for Linear SVM\n",
    "    # We focus on smaller C values to enforce a \"Wide Margin\"\n",
    "    param_grid = {\n",
    "        'svm__C': [0.001, 0.01, 0.1, 1, 5, 10]\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)\n",
    "    \n",
    "    print(\"Tuning Linear SVM...\")\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best Parameters: {grid.best_params_}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_train)\n",
    "    \n",
    "    print(\"\\n--- Training Performance ---\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_train, y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_train, y_pred):.4f}\")\n",
    "    \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d042d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 4: Testing Function\n",
    "# =========================================================================================\n",
    "\n",
    "def testing_function(path, model):\n",
    "    \"\"\"\n",
    "    Tests the trained pipeline on new data.\n",
    "    The pipeline automatically applies the same Scaling and Feature Selection.\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading testing data from: {path}\")\n",
    "    X_test, y_test = get_data_stage2(path)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\n--- Testing Performance (Stage 2) ---\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    return y_test, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "54e8ee2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== STAGE 2: FEATURE ENGINEERING + SELECTION ==========\n",
      "Loading training data from: Lego_dataset_2/training/\n",
      "Tuning Linear SVM...\n",
      "Best Parameters: {'svm__C': 10}\n",
      "\n",
      "--- Training Performance ---\n",
      "Confusion Matrix:\n",
      "[[27  0  0  0]\n",
      " [ 0 25  0  2]\n",
      " [ 0  0 27  0]\n",
      " [ 0  0  0 27]]\n",
      "Accuracy: 0.9815\n",
      "\n",
      "Loading testing data from: Lego_dataset_2/testing/\n",
      "\n",
      "--- Testing Performance (Stage 2) ---\n",
      "Confusion Matrix:\n",
      "[[19  3  0  5]\n",
      " [ 0 23  0  4]\n",
      " [ 1  1 25  0]\n",
      " [ 2  0  0 25]]\n",
      "Accuracy: 0.8519\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================================\n",
    "# Cell 5: Main Execution\n",
    "# =========================================================================================\n",
    "\n",
    "# Define Paths (UPDATE THESE TO YOUR ACTUAL FOLDER PATHS)\n",
    "TRAIN_PATH = 'Lego_dataset_2/training/'\n",
    "TEST_PATH  = 'Lego_dataset_2/testing/'\n",
    "\n",
    "print(\"========== STAGE 2: FEATURE ENGINEERING + SELECTION ==========\")\n",
    "\n",
    "try:\n",
    "    # 1. Train Model (includes SFS)\n",
    "    stage2_model = training_function(TRAIN_PATH)\n",
    "\n",
    "    # 2. Test Model\n",
    "    y_test, y_pred = testing_function(TEST_PATH, stage2_model)\n",
    "    \n",
    "except Exception as e:\n",
    "     print(f\"Could not run Task (check paths or data): {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
