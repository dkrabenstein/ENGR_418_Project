{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea46b610",
   "metadata": {},
   "source": [
    "ENGR 418 - Group 33\\\n",
    "Matthew Ofina - 84790435\\\n",
    "Duncan Raenstein - 57709628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30591f4b-d627-4066-b8bb-306185c2a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\dkrab\\onedrive - ubc\\documents\\ubco engineering\\year 6 semester 1\\engr 418\\project\\engr_418_project\\.venv\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\dkrab\\onedrive - ubc\\documents\\ubco engineering\\year 6 semester 1\\engr 418\\project\\engr_418_project\\.venv\\lib\\site-packages (from opencv-python) (2.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac3cf607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# This cell imports necassary libraries\n",
    "# ============================================================\n",
    "\n",
    "# Basic imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c315c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# This cell is the feature engineering cell and extracts features. It also contains helper\n",
    "# functions to load data and get labels.\n",
    "# =========================================================================================\n",
    "\n",
    "def get_label(file_name):\n",
    "    \"\"\"\n",
    "    This function extracts the label from the file name.\n",
    "    \"\"\"\n",
    "    if file_name.startswith('2'): return 0\n",
    "    elif file_name.startswith('c'): return 1\n",
    "    elif file_name.startswith('r'): return 2\n",
    "    elif file_name.startswith('s'): return 3\n",
    "    return 0\n",
    "\n",
    "def extract_engineered_features_opencv(im_array):\n",
    "    \"\"\"\n",
    "    Extracts 12 Base Features:\n",
    "    - 5 Geometric (Area, Compactness, Solidity, Aspect Ratio, Rectangularity)\n",
    "    - 7 Hu Moments (Shape Invariants)\n",
    "    \"\"\"\n",
    "\n",
    "    # Makes sure data is the corret type\n",
    "    if im_array.dtype != np.uint8:\n",
    "        im_array = im_array.astype(np.uint8)\n",
    "\n",
    "    # Preprocessing, converts to binary\n",
    "    inv_im = cv2.bitwise_not(im_array)\n",
    "    _, binary = cv2.threshold(inv_im, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Strong morphological closing (5x5) to solidify shapes\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # Converts image to contour format\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours: return np.zeros(12)\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Extracts 5 Geometric Features\n",
    "    # Area\n",
    "    area = cv2.contourArea(cnt)\n",
    "    \n",
    "    # Compactness (P^2 / A)\n",
    "    perimeter = cv2.arcLength(cnt, True)\n",
    "    compactness = (perimeter ** 2) / area if area > 0 else 0\n",
    "\n",
    "    # Solidity (Area / Convex Hull)\n",
    "    hull = cv2.convexHull(cnt)\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    solidity = area / hull_area if hull_area > 0 else 0\n",
    "    \n",
    "    # Aspect Ratio (Rotation Invariant)\n",
    "    rect = cv2.minAreaRect(cnt) \n",
    "    (center), (width, height), angle = rect\n",
    "    dims = sorted([width, height])\n",
    "    aspect_ratio = dims[1] / dims[0] if dims[0] > 0 else 0\n",
    "    \n",
    "    # Rectangularity (w / h)\n",
    "    rect_area = width * height\n",
    "    rectangularity = area / rect_area if rect_area > 0 else 0\n",
    "\n",
    "    # Extracts 7 Hu Moments\n",
    "    moments = cv2.moments(cnt)\n",
    "    hu = cv2.HuMoments(moments).flatten()\n",
    "\n",
    "    # Log scale for numerical stability\n",
    "    hu_log = [ -1 * np.sign(h) * np.log10(np.abs(h) + 1e-7) for h in hu ]\n",
    "    \n",
    "    # Creates a combined Feature Vector with all of the previously extracted features\n",
    "    features = np.array([\n",
    "        area, compactness, solidity, aspect_ratio, rectangularity,\n",
    "        hu_log[0], hu_log[1], hu_log[2], hu_log[3], hu_log[4], hu_log[5], hu_log[6]\n",
    "    ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def get_data_stage2(folder_path):\n",
    "    \"\"\"\n",
    "    This fuction loads all images from the specified folder and extracts features and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lists all image files in the folder\n",
    "    file_names = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg'))]\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # iterates through each file, extracts features and labels\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        im = Image.open(file_path).convert('L')\n",
    "        im_array = np.array(im)\n",
    "        feats = extract_engineered_features_opencv(im_array)\n",
    "        lbl = get_label(file_name)\n",
    "        features_list.append(feats)\n",
    "        labels_list.append(lbl)\n",
    "        \n",
    "    # Converts lists to numpy arrays and returns\n",
    "    return np.array(features_list), np.array(labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d071f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Training cell with L1 regularization\n",
    "# =========================================================================================\n",
    "\n",
    "def training_function(path):\n",
    "    \"\"\"\n",
    "    This function inputs the extracted features into a Logistic Regression pipeline with:\n",
    "    1. Scaling (StandardScaler)\n",
    "    2. Poly Expansion (12 Base -> ~90 Interactions)\n",
    "    3. SelectKBest (Keeps Top 10) - REDUCED from 40 to prevent overfitting.\n",
    "    4. Logistic Regression with L1 Penalty (Lasso)\n",
    "       - L1 forces weak features to 0, acting as a second layer of feature selection.\n",
    "    It then fits a model and prints the training performance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Uses function to retrieve features from path\n",
    "    X_train, y_train = get_data_stage2(path)\n",
    "    \n",
    "    # Construct Pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly', PolynomialFeatures(degree=2)), \n",
    "        ('select', SelectKBest(score_func=f_classif, k=34)),\n",
    "        ('clf', LogisticRegression(\n",
    "            penalty='l1',       # L1 Lasso Regularization\n",
    "            solver='liblinear', # Solver that supports L1\n",
    "            max_iter=5000,      # Iterations convergence function run\n",
    "            multi_class='ovr'   # One-vs-Rest for Liblinear\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Grid Search Parameters, We focus on SMALL C values to enforce strong regularization (generalization)\n",
    "    param_grid = {\n",
    "        'clf__C': [0.01, 0.05, 100, 50, 10] \n",
    "    }\n",
    "    \n",
    "    # Uses grid search with 5-fold cross-validation to find best C\n",
    "    grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=1)\n",
    "    \n",
    "    # Fits Model\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluates to find the best model\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_train)\n",
    "    \n",
    "    # Prints Training Performance\n",
    "    print(\"\\n--- Training Performance (L1 Optimized) ---\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_train, y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_train, y_pred):.4f}\")\n",
    "    \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859b88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Testing function cell required for grading\n",
    "# =========================================================================================\n",
    "\n",
    "def testing_function(path, model):\n",
    "    \"\"\"\n",
    "    Tests the trained Logistic Regression pipeline on new data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieves test data and extracts features\n",
    "    X_test, y_test = get_data_stage2(path)\n",
    "    \n",
    "    # Makes predictions based on model from training\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Prints evaluation\n",
    "    print(\"\\n--- Testing Performance ---\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10b0d089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Performance (L1 Optimized) ---\n",
      "Confusion Matrix:\n",
      "[[27  0  0  0]\n",
      " [ 0 27  0  0]\n",
      " [ 0  0 27  0]\n",
      " [ 0  0  0 27]]\n",
      "Accuracy: 1.0000\n",
      "\n",
      "--- Testing Performance ---\n",
      "Confusion Matrix:\n",
      "[[22  3  0  2]\n",
      " [ 0 26  0  1]\n",
      " [ 1  0 25  1]\n",
      " [ 0  0  0 27]]\n",
      "Accuracy: 0.9259\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================================\n",
    "# Trains and tests the model\n",
    "# =========================================================================================\n",
    "\n",
    "# UPDATE THESE PATHS TO YOUR LOCAL FOLDERS\n",
    "TRAIN_PATH = 'Lego_dataset_2/training/'\n",
    "TEST_PATH  = 'Lego_dataset_2/testing/'\n",
    "\n",
    "# Trains model\n",
    "stage2_model = training_function(TRAIN_PATH)\n",
    "\n",
    "# Tests model\n",
    "testing_function(TEST_PATH, stage2_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
