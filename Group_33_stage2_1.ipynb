{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ac3cf607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 1: Group Information and Imports\n",
    "# =========================================================================================\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning Imports\n",
    "from sklearn.svm import SVC                   # Support Vector Classifier\n",
    "from sklearn.model_selection import GridSearchCV # Automated Parameter Tuning\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d071f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 2: Physics-Based Feature Engineering (5 Robust Features)\n",
    "# =========================================================================================\n",
    "\n",
    "def get_label(file_name):\n",
    "    if file_name.startswith('2'): return 0\n",
    "    elif file_name.startswith('c'): return 1\n",
    "    elif file_name.startswith('r'): return 2\n",
    "    elif file_name.startswith('s'): return 3\n",
    "    return 0\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_engineered_features_opencv(im_array):\n",
    "    \"\"\"\n",
    "    Extracts physically descriptive features including:\n",
    "    1. Shape features (Area, Compactness, Solidity, Aspect Ratio, Rectangularity)\n",
    "    2. Hu Moments (7 shape invariants)\n",
    "    3. Color Histogram (HSV color distribution)\n",
    "    \"\"\"\n",
    "    # Ensure correct data type\n",
    "    if im_array.dtype != np.uint8:\n",
    "        im_array = im_array.astype(np.uint8)\n",
    "\n",
    "    # --- Preprocessing for Shape Analysis ---\n",
    "    # We use grayscale for contour detection\n",
    "    if len(im_array.shape) == 3:\n",
    "        gray = cv2.cvtColor(im_array, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = im_array\n",
    "\n",
    "    # Invert (assuming white background) and threshold\n",
    "    inv_im = cv2.bitwise_not(gray)\n",
    "    _, binary = cv2.threshold(inv_im, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Cleanup noise\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Fallback if no object detected (returns array of correct size)\n",
    "    # 5 basic + 7 Hu + 24 Color (8*3) = 36 features\n",
    "    if not contours: \n",
    "        return np.zeros(36)\n",
    "    \n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # ============================\n",
    "    # Group 1: Geometric Features\n",
    "    # ============================\n",
    "    \n",
    "    # 1. Area\n",
    "    area = cv2.contourArea(cnt)\n",
    "    \n",
    "    # 2. Compactness\n",
    "    perimeter = cv2.arcLength(cnt, True)\n",
    "    compactness = (perimeter ** 2) / area if area > 0 else 0\n",
    "\n",
    "    # 3. Solidity\n",
    "    hull = cv2.convexHull(cnt)\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    solidity = area / hull_area if hull_area > 0 else 0\n",
    "    \n",
    "    # 4. Aspect Ratio (Rotation Invariant via minAreaRect)\n",
    "    rect = cv2.minAreaRect(cnt) \n",
    "    (center), (width, height), angle = rect\n",
    "    dims = sorted([width, height])\n",
    "    aspect_ratio = dims[1] / dims[0] if dims[0] > 0 else 0\n",
    "    \n",
    "    # 5. Rectangularity\n",
    "    rect_area = width * height\n",
    "    rectangularity = area / rect_area if rect_area > 0 else 0\n",
    "\n",
    "    basic_features = np.array([area, compactness, solidity, aspect_ratio, rectangularity])\n",
    "\n",
    "    # ============================\n",
    "    # Group 2: Hu Moments (Shape Invariants)\n",
    "    # ============================\n",
    "    # Hu Moments are excellent for rotation invariance.\n",
    "    # We use moments of the *contour* rather than the image for speed.\n",
    "    moments = cv2.moments(cnt)\n",
    "    hu = cv2.HuMoments(moments).flatten()\n",
    "    \n",
    "    # Log scale transform: Raw Hu moments are tiny (e.g., 10^-7). \n",
    "    # Log scaling brings them into a usable range for the classifier.\n",
    "    hu_moments = -np.sign(hu) * np.log10(np.abs(hu) + 1e-10)\n",
    "\n",
    "    # ============================\n",
    "    # Combine All Features\n",
    "    # ============================\n",
    "    return np.hstack([basic_features, hu_moments])\n",
    "\n",
    "def get_data_stage2(folder_path):\n",
    "    file_names = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg'))]\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        im = Image.open(file_path).convert('L')\n",
    "        im_array = np.array(im)\n",
    "        feats = extract_engineered_features_opencv(im_array)\n",
    "        lbl = get_label(file_name)\n",
    "        features_list.append(feats)\n",
    "        labels_list.append(lbl)\n",
    "        \n",
    "    return np.array(features_list), np.array(labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "859b88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 3: Training Function (Linear SVM for Generalization)\n",
    "# =========================================================================================\n",
    "\n",
    "def training_function(path):\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "    1. Scaling (StandardScaler)\n",
    "    2. Linear SVM\n",
    "    \"\"\"\n",
    "    print(f\"Loading training data from: {path}\")\n",
    "    X_train, y_train = get_data_stage2(path)\n",
    "    # Pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(kernel='linear', probability=True))\n",
    "    ])\n",
    "    \n",
    "    # Grid Search for Linear SVM\n",
    "    # We focus on smaller C values to enforce a \"Wide Margin\"\n",
    "    param_grid = {\n",
    "        'svm__C': [0.001, 0.01, 0.01, 0.1, 0.5, 0.8]\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)\n",
    "    \n",
    "    print(\"Tuning Linear SVM...\")\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best Parameters: {grid.best_params_}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_train)\n",
    "    \n",
    "    print(\"\\n--- Training Performance ---\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_train, y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_train, y_pred):.4f}\")\n",
    "    \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "10b0d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Cell 4: Testing Function\n",
    "# =========================================================================================\n",
    "\n",
    "def testing_function(path, model):\n",
    "    \"\"\"\n",
    "    Tests the trained pipeline on new data.\n",
    "    The pipeline automatically applies the same Scaling and Feature Selection.\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading testing data from: {path}\")\n",
    "    X_test, y_test = get_data_stage2(path)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\n--- Testing Performance (Stage 2) ---\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    return y_test, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9acb1dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== STAGE 2: FEATURE ENGINEERING + SELECTION ==========\n",
      "Loading training data from: Lego_dataset_2/training/\n",
      "Tuning Linear SVM...\n",
      "Best Parameters: {'svm__C': 0.8}\n",
      "\n",
      "--- Training Performance ---\n",
      "Confusion Matrix:\n",
      "[[27  0  0  0]\n",
      " [ 1 26  0  0]\n",
      " [ 0  0 27  0]\n",
      " [ 0  0  0 27]]\n",
      "Accuracy: 0.9907\n",
      "\n",
      "Loading testing data from: Lego_dataset_2/testing/\n",
      "\n",
      "--- Testing Performance (Stage 2) ---\n",
      "Confusion Matrix:\n",
      "[[19  5  0  3]\n",
      " [ 0 26  0  1]\n",
      " [ 0  0 27  0]\n",
      " [ 1  0  0 26]]\n",
      "Accuracy: 0.9074\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================================\n",
    "# Cell 5: Main Execution\n",
    "# =========================================================================================\n",
    "\n",
    "# Define Paths (UPDATE THESE TO YOUR ACTUAL FOLDER PATHS)\n",
    "TRAIN_PATH = 'Lego_dataset_2/training/'\n",
    "TEST_PATH  = 'Lego_dataset_2/testing/'\n",
    "\n",
    "print(\"========== STAGE 2: FEATURE ENGINEERING + SELECTION ==========\")\n",
    "\n",
    "try:\n",
    "    # 1. Train Model (includes SFS)\n",
    "    stage2_model = training_function(TRAIN_PATH)\n",
    "\n",
    "    # 2. Test Model\n",
    "    y_test, y_pred = testing_function(TEST_PATH, stage2_model)\n",
    "    \n",
    "except Exception as e:\n",
    "     print(f\"Could not run Task (check paths or data): {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
